{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-16T19:23:18.864372Z","iopub.execute_input":"2023-02-16T19:23:18.864947Z","iopub.status.idle":"2023-02-16T19:24:07.130711Z","shell.execute_reply.started":"2023-02-16T19:23:18.864840Z","shell.execute_reply":"2023-02-16T19:24:07.128856Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nCollecting pyspark\n  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824024 sha256=2429fc7f727391d69b3809b5fd2a93d35add9d4d0c4e6221296b2cd7c51125e1\n  Stored in directory: /root/.cache/pip/wheels/07/fb/67/b9f2c0242d156eaa136b45ae4fd99d3e7c0ecc2acfd26f47b9\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# What is Parquet File?\n**Apache Parquet** file is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or programming language.","metadata":{}},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\nspark=SparkSession.builder.appName(\"parquetFile\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:24:07.133265Z","iopub.execute_input":"2023-02-16T19:24:07.133697Z","iopub.status.idle":"2023-02-16T19:24:13.817536Z","shell.execute_reply.started":"2023-02-16T19:24:07.133646Z","shell.execute_reply":"2023-02-16T19:24:13.815949Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by bash)\nbash: /opt/conda/lib/libtinfo.so.6: no version information available (required by bash)\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"23/02/16 19:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"data =[(\"Hanan \",\"\",\"Ali\",\"HTML\",\"M\",3000),\n              (\"Doaa \",\"Amad\",\"\",\"PHP\",\"M\",4000),\n              (\"Nema \",\"Hassin\",\"abdallrahman\",\"PYTHON\",\"M\",4000),\n              (\"gamal \",\"yossef\",\"abdallh\",\"C++\",\"F\",4000),\n              (\"morad\",\"eslam\",\"abas\",\"Docker\",\"F\",-1)]\ncolumns=[\"firstname\",\"middlename\",\"lastname\",\"skills\",\"gender\",\"salary\"]\ndf=spark.createDataFrame(data,columns)\ndf.printSchema()\ndf.show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:24:34.840790Z","iopub.execute_input":"2023-02-16T19:24:34.841214Z","iopub.status.idle":"2023-02-16T19:24:35.274399Z","shell.execute_reply.started":"2023-02-16T19:24:34.841180Z","shell.execute_reply":"2023-02-16T19:24:35.272941Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- skills: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---------+----------+------------+------+------+------+\n|firstname|middlename|lastname    |skills|gender|salary|\n+---------+----------+------------+------+------+------+\n|Hanan    |          |Ali         |HTML  |M     |3000  |\n|Doaa     |Amad      |            |PHP   |M     |4000  |\n|Nema     |Hassin    |abdallrahman|PYTHON|M     |4000  |\n|gamal    |yossef    |abdallh     |C++   |F     |4000  |\n|morad    |eslam     |abas        |Docker|F     |-1    |\n+---------+----------+------------+------+------+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Pyspark Write DataFrame to Parquet file format\n","metadata":{}},{"cell_type":"code","source":"\ndf.write.parquet(\"/kaggle/working/people.parquet\")\n","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:24:20.598928Z","iopub.execute_input":"2023-02-16T19:24:20.599456Z","iopub.status.idle":"2023-02-16T19:24:22.155410Z","shell.execute_reply.started":"2023-02-16T19:24:20.599402Z","shell.execute_reply":"2023-02-16T19:24:22.152646Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Pyspark Read Parquet file into DataFrame\n","metadata":{}},{"cell_type":"code","source":"\nparDF=spark.read.parquet(\"/kaggle/working/people.parquet\")\nparDF.printSchema()\nparDF.show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:24:55.803339Z","iopub.execute_input":"2023-02-16T19:24:55.803747Z","iopub.status.idle":"2023-02-16T19:24:56.999786Z","shell.execute_reply.started":"2023-02-16T19:24:55.803714Z","shell.execute_reply":"2023-02-16T19:24:56.998575Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- skills: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---------+----------+------------+------+------+------+\n|firstname|middlename|lastname    |skills|gender|salary|\n+---------+----------+------------+------+------+------+\n|Nema     |Hassin    |abdallrahman|PYTHON|M     |4000  |\n|gamal    |yossef    |abdallh     |C++   |F     |4000  |\n|morad    |eslam     |abas        |Docker|F     |-1    |\n|Hanan    |          |Ali         |HTML  |M     |3000  |\n|Doaa     |Amad      |            |PHP   |M     |4000  |\n+---------+----------+------------+------+------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"parDF1=spark.read.parquet(\"/kaggle/working/people.parquet/part-00000-2d867555-03dd-420e-9967-437ddc7fbdb6-c000.snappy.parquet\")\nparDF1.printSchema()\nparDF1.show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:25:38.305961Z","iopub.execute_input":"2023-02-16T19:25:38.306441Z","iopub.status.idle":"2023-02-16T19:25:38.629110Z","shell.execute_reply.started":"2023-02-16T19:25:38.306397Z","shell.execute_reply":"2023-02-16T19:25:38.627772Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- skills: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---------+----------+--------+------+------+------+\n|firstname|middlename|lastname|skills|gender|salary|\n+---------+----------+--------+------+------+------+\n|Hanan    |          |Ali     |HTML  |M     |3000  |\n+---------+----------+--------+------+------+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Append or Overwrite an existing Parquet file\nUsing append save mode, you can append a dataframe to an existing parquet file. Incase to overwrite use overwrite save mode.","metadata":{}},{"cell_type":"code","source":"df.write.mode('append').parquet(\"/kaggle/working/people.parquet\")\n# df.write.mode('overwrite').parquet(\"/kaggle/working/people.parquet\")","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:27:12.971236Z","iopub.execute_input":"2023-02-16T19:27:12.971699Z","iopub.status.idle":"2023-02-16T19:27:13.211633Z","shell.execute_reply.started":"2023-02-16T19:27:12.971663Z","shell.execute_reply":"2023-02-16T19:27:13.210678Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"parDF2=spark.read.parquet(\"/kaggle/working/people.parquet\")\nparDF2.printSchema()\nparDF2.show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:27:14.491488Z","iopub.execute_input":"2023-02-16T19:27:14.491952Z","iopub.status.idle":"2023-02-16T19:27:14.668232Z","shell.execute_reply.started":"2023-02-16T19:27:14.491917Z","shell.execute_reply":"2023-02-16T19:27:14.666841Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- skills: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---------+----------+------------+------+------+------+\n|firstname|middlename|lastname    |skills|gender|salary|\n+---------+----------+------------+------+------+------+\n|Nema     |Hassin    |abdallrahman|PYTHON|M     |4000  |\n|Nema     |Hassin    |abdallrahman|PYTHON|M     |4000  |\n|Nema     |Hassin    |abdallrahman|PYTHON|M     |4000  |\n|gamal    |yossef    |abdallh     |C++   |F     |4000  |\n|morad    |eslam     |abas        |Docker|F     |-1    |\n|gamal    |yossef    |abdallh     |C++   |F     |4000  |\n|morad    |eslam     |abas        |Docker|F     |-1    |\n|gamal    |yossef    |abdallh     |C++   |F     |4000  |\n|morad    |eslam     |abas        |Docker|F     |-1    |\n|Hanan    |          |Ali         |HTML  |M     |3000  |\n|Hanan    |          |Ali         |HTML  |M     |3000  |\n|Hanan    |          |Ali         |HTML  |M     |3000  |\n|Doaa     |Amad      |            |PHP   |M     |4000  |\n|Doaa     |Amad      |            |PHP   |M     |4000  |\n|Doaa     |Amad      |            |PHP   |M     |4000  |\n+---------+----------+------------+------+------+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Executing SQL queries DataFrame\nPyspark Sql provides to create temporary views on parquet files for executing sql queries. These views are available until your program exists.","metadata":{}},{"cell_type":"code","source":"\nparDF2.createOrReplaceTempView(\"ParquetTable\")\nparkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:27:26.975228Z","iopub.execute_input":"2023-02-16T19:27:26.975658Z","iopub.status.idle":"2023-02-16T19:27:27.122538Z","shell.execute_reply.started":"2023-02-16T19:27:26.975613Z","shell.execute_reply":"2023-02-16T19:27:27.121208Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"parkSQL.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:27:27.420412Z","iopub.execute_input":"2023-02-16T19:27:27.420933Z","iopub.status.idle":"2023-02-16T19:27:27.746905Z","shell.execute_reply.started":"2023-02-16T19:27:27.420890Z","shell.execute_reply":"2023-02-16T19:27:27.745611Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"+---------+----------+------------+------+------+------+\n|firstname|middlename|    lastname|skills|gender|salary|\n+---------+----------+------------+------+------+------+\n|    Nema |    Hassin|abdallrahman|PYTHON|     M|  4000|\n|    Nema |    Hassin|abdallrahman|PYTHON|     M|  4000|\n|    Nema |    Hassin|abdallrahman|PYTHON|     M|  4000|\n|   gamal |    yossef|     abdallh|   C++|     F|  4000|\n|   gamal |    yossef|     abdallh|   C++|     F|  4000|\n|   gamal |    yossef|     abdallh|   C++|     F|  4000|\n|    Doaa |      Amad|            |   PHP|     M|  4000|\n|    Doaa |      Amad|            |   PHP|     M|  4000|\n|    Doaa |      Amad|            |   PHP|     M|  4000|\n+---------+----------+------------+------+------+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Create Parquet partition file\nWhen we execute a particular query on the PERSON table, it scan’s through all the rows and returns the results back. This is similar to the traditional database query execution. In PySpark, we can improve query execution in an optimized way by doing partitions on the data using pyspark partitionBy() method. Following is the example of partitionBy().","metadata":{}},{"cell_type":"markdown","source":"When you check the people2.parquet file, it has two partitions “gender” followed by “salary” inside.","metadata":{}},{"cell_type":"code","source":"\ndf.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/kaggle/working/people1.parquet\")\n","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:27:48.829612Z","iopub.execute_input":"2023-02-16T19:27:48.830116Z","iopub.status.idle":"2023-02-16T19:27:49.531487Z","shell.execute_reply.started":"2023-02-16T19:27:48.830058Z","shell.execute_reply":"2023-02-16T19:27:49.530102Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### Retrieving from a partitioned Parquet file\nThe example below explains of reading partitioned parquet file into DataFrame with **gender=M**.","metadata":{}},{"cell_type":"code","source":"\nparDF3=spark.read.parquet(\"/kaggle/working/people1.parquet/gender=M\")\nparDF3.show(truncate=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:27:56.629171Z","iopub.execute_input":"2023-02-16T19:27:56.629683Z","iopub.status.idle":"2023-02-16T19:27:56.894231Z","shell.execute_reply.started":"2023-02-16T19:27:56.629629Z","shell.execute_reply":"2023-02-16T19:27:56.892786Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"+---------+----------+------------+------+------+\n|firstname|middlename|lastname    |skills|salary|\n+---------+----------+------------+------+------+\n|Nema     |Hassin    |abdallrahman|PYTHON|4000  |\n|Hanan    |          |Ali         |HTML  |3000  |\n|Doaa     |Amad      |            |PHP   |4000  |\n+---------+----------+------------+------+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Retrieving from a partitioned Parquet file\nThe example below explains of reading partitioned parquet file into DataFrame with **gender=F**.","metadata":{}},{"cell_type":"code","source":"parDF4=spark.read.parquet(\"/kaggle/working/people1.parquet/gender=F\")\nparDF4.show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-16T19:28:41.082557Z","iopub.execute_input":"2023-02-16T19:28:41.083035Z","iopub.status.idle":"2023-02-16T19:28:41.234776Z","shell.execute_reply.started":"2023-02-16T19:28:41.082999Z","shell.execute_reply":"2023-02-16T19:28:41.233392Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"+---------+----------+--------+------+------+\n|firstname|middlename|lastname|skills|salary|\n+---------+----------+--------+------+------+\n|gamal    |yossef    |abdallh |C++   |4000  |\n|morad    |eslam     |abas    |Docker|-1    |\n+---------+----------+--------+------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}